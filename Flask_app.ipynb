{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy.spatial.distance as dist\n",
    "import re\n",
    "import psycopg2\n",
    "from sklearn.externals import joblib\n",
    "import pandas.io.sql as sqlio\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\popak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\popak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# URL Removal\n",
    "def remove_urls(articles):\n",
    "    removedUrlArticles = []\n",
    "    for article in articles:\n",
    "        removedUrlArticles.append(re.sub('http\\S+', '', article))\n",
    "    return removedUrlArticles\n",
    "\n",
    "\n",
    "# Lower Case Conversion\n",
    "def convert_to_lower(articles):\n",
    "    lowerArticles = []\n",
    "    for article in articles:\n",
    "        lowerArticles.append(article.lower())\n",
    "    return lowerArticles\n",
    "\n",
    "\n",
    "# Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the set of stop words the first time\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def remove_stopwords(articles):\n",
    "    noStopWordArticles = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) # Set improves performance\n",
    "  \n",
    "    for article in articles:  \n",
    "        word_tokens = word_tokenize(article) \n",
    "        filtered_article = [word for word in word_tokens if not word in stop_words] \n",
    "        filteredArticleString = ' '.join(word for word in filtered_article)\n",
    "        noStopWordArticles.append(filteredArticleString)\n",
    "        #print(tweet)\n",
    "        #print(filtered_tweet)    \n",
    "        #print (filteredTweetString)\n",
    "    return noStopWordArticles\n",
    "\n",
    "\n",
    "# Character removal\n",
    "def remove_unwanted_characters(articles):\n",
    "    unwantedChars = '''()-[]{};:'\"\\,<>./@#$%^&*_~1234567890'''\n",
    "    cleanArticles = []\n",
    "    for article in articles:\n",
    "        for punc in list(unwantedChars):\n",
    "            article = article.replace(punc,'')\n",
    "        cleanArticles.append(article)\n",
    "    return cleanArticles\n",
    "\n",
    "\n",
    "def list_to_str(alist):\n",
    "    string_list = [str(i) for i in alist]\n",
    "    return '{'+','.join(string_list)+'}'\n",
    "\n",
    "\n",
    "\n",
    "def initialize_DB(file):\n",
    "    \n",
    "    try:\n",
    "        rawData = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "        data = rawData[['author', 'link', 'title', 'text']]\n",
    "        data.drop_duplicates(subset =\"title\", keep = False, inplace = True) \n",
    "\n",
    "        articles = data['text'].tolist()\n",
    "\n",
    "        articles = remove_urls(articles)\n",
    "        articles = convert_to_lower(articles)\n",
    "        articles = remove_stopwords(articles)\n",
    "        articles = remove_unwanted_characters(articles)\n",
    "        data['author'] = remove_unwanted_characters(data['author'])\n",
    "        data['title'] = remove_unwanted_characters(data['title'])\n",
    "\n",
    "        tfidfVec = TfidfVectorizer()\n",
    "        text_tfidf = tfidfVec.fit_transform(articles)\n",
    "        text_tfidf = text_tfidf.toarray()\n",
    "        data['tf_idf'] = text_tfidf.tolist()\n",
    "\n",
    "        joblib.dump(tfidfVec, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "\n",
    "        #connect to the db\n",
    "        con = psycopg2.connect(\n",
    "            host = 'localhost',\n",
    "            database = 'Article',\n",
    "            user = 'jim',\n",
    "            password = 'postgre'\n",
    "        )\n",
    "\n",
    "        #cursor\n",
    "        cur = con.cursor()\n",
    "\n",
    "\n",
    "        cur.execute(f\"CREATE TABLE articles (article_id int  primary key NOT NULL, author varchar(255), title varchar(255), article_link text, tf_idf float[])\")\n",
    "\n",
    "\n",
    "        #execute query\n",
    "        for index, row in data.iterrows():\n",
    "            cur.execute(f\"insert into articles (article_id, author, title, article_link, tf_idf) values ({index}, '{row['author']}' , '{row['title']}', '{row['link']}', '{list_to_str(row['tf_idf'])}')\")\n",
    "\n",
    "\n",
    "\n",
    "        #commit the transaction\n",
    "        con.commit()\n",
    "\n",
    "        #close the cursor\n",
    "        cur.close()\n",
    "\n",
    "        #close the connection\n",
    "        con.close()\n",
    "        \n",
    "    except:\n",
    "        print('DB already initialized')\n",
    "        \n",
    "    database = retrieve_db()\n",
    "    vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "    \n",
    "    return database, vectorizer\n",
    "    \n",
    "    \n",
    "def retrieve_db():\n",
    "    \n",
    "    #connect to the db\n",
    "    con = psycopg2.connect(\n",
    "        host = 'localhost',\n",
    "        database = 'Article',\n",
    "        user = 'jim',\n",
    "        password = 'postgre'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #execute query\n",
    "    sql = 'select * from articles;'\n",
    "    \n",
    "    data = sqlio.read_sql_query(sql, con)\n",
    "    \n",
    "\n",
    "    #close the connection\n",
    "    con.close()\n",
    "    \n",
    "    return data\n",
    "\n",
    "    \n",
    "    \n",
    "def retrieve_relevant_articles(query, k, distance, vectorizer, database):\n",
    "    \n",
    "    query = query.lower()\n",
    "    lst = [query]\n",
    "    series = pd.Series(lst)\n",
    "    query_tfidf = vectorizer.transform(series)\n",
    "    query_tfidf = query_tfidf.toarray()\n",
    "    \n",
    "    \n",
    "    if distance == 'euclidean':\n",
    "        euclidean_dist_list = []\n",
    "        for vector in database['tf_idf']:\n",
    "            euclidean_dist_list.append(dist.euclidean(vector, query_tfidf))\n",
    "        database[\"euclidean_dist\"] = euclidean_dist_list\n",
    "        relevant_articles = database.sort_values(by=['euclidean_dist'])[0:k]\n",
    "     \n",
    "    \n",
    "    elif distance == 'minkowski':\n",
    "        minkowski_dist_list = []\n",
    "        for vector in database['tf_idf']:\n",
    "            minkowski_dist_list.append(dist.minkowski(vector, query_tfidf))\n",
    "        database[\"minkowski\"] = minkowski_dist_list\n",
    "        relevant_articles = database.sort_values(by=['minkowski'])[0:k]\n",
    "    \n",
    "    \n",
    "    elif distance == 'chebyshev':\n",
    "        chebyshev_dist_list = []\n",
    "        for vector in database['tf_idf']:\n",
    "            chebyshev_dist_list.append(dist.chebyshev(vector, query_tfidf))\n",
    "        database[\"chebyshev_dist\"] = chebyshev_dist_list\n",
    "        relevant_articles = database.sort_values(by=['chebyshev_dist'])[0:k]\n",
    "        \n",
    "        \n",
    "    elif distance == 'dice':\n",
    "        dice_dist_list = []\n",
    "        for vector in database['tf_idf']:\n",
    "            dice_dist_list.append(dist.dice(vector, query_tfidf))\n",
    "        database[\"dice_dist\"] = dice_dist_list\n",
    "        relevant_articles = database.sort_values(by=['dice_dist'])[0:k]\n",
    "        \n",
    "        \n",
    "    elif distance == 'cosine':\n",
    "        cosine_dist_list = []\n",
    "        for vector in database['tf_idf']:\n",
    "            cosine_dist_list.append(dist.cosine(vector, query_tfidf))\n",
    "        database[\"cosine_dist\"] = cosine_dist_list\n",
    "        relevant_articles = database.sort_values(by=['cosine_dist'])[0:k]\n",
    "\n",
    "    \n",
    "    \n",
    "    return relevant_articles\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_dict (results): \n",
    "    return {'result' : results[[\"author\",\"title\",\"article_link\"]].to_dict('records')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\popak\\anaconda3\\envs\\project_app\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\popak\\anaconda3\\envs\\project_app\\lib\\site-packages\\ipykernel_launcher.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\popak\\anaconda3\\envs\\project_app\\lib\\site-packages\\ipykernel_launcher.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\popak\\anaconda3\\envs\\project_app\\lib\\site-packages\\ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB already initialized\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [16/Jun/2020 21:29:59] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:30:37] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:37:46] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:38:19] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:41:11] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:49:33] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:50:23] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:50:28] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:50:50] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:57:01] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 21:58:17] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:03:04] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:09:09] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:09:14] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:17:23] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:19:34] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:19:39] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:19:45] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:20:04] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:24:35] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:26:43] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:26:49] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [16/Jun/2020 22:33:01] \"\u001b[37mPOST /post HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, current_app, request, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "database, vectorizer = initialize_DB('articles.csv')\n",
    "\n",
    "@app.route('/')\n",
    "def home_page():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/post', methods = ['POST'])\n",
    "def post():\n",
    "    data = request.form\n",
    "    results = retrieve_relevant_articles(data['query'], int(data['k']), data['distance'], vectorizer, database)\n",
    "    results_dict = convert_to_dict(results)\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "def shutdown_server():\n",
    "    func = request.environ.get('werkzeug.server.shutdown')\n",
    "    if func is None:\n",
    "        raise RuntimeError('Not running with the Werkzeug Server')\n",
    "    func()\n",
    "@app.route('/shutdown', methods=['GET'])\n",
    "def shutdown():\n",
    "    shutdown_server()\n",
    "    return 'Server shutting down...'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
